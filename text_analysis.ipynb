{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import regex as re\n",
    "import string\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "#import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "#nlp = en_core_web_sm.load()\n",
    "#nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# #alter default spacy stopwords so 'not' isn't considered a stop word\n",
    "\n",
    "    \n",
    "\n",
    "# #Should hopefully deal with contractions\n",
    "# special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# def custom_tokenizer(nlp, custom_stop_words):\n",
    "    \n",
    "#     for w in customize_stop_words:\n",
    "#         nlp.vocab[w].is_stop = False\n",
    "        \n",
    "#     return Tokenizer(nlp.vocab, rules=special_cases)\n",
    "\n",
    "# nlp.tokenizer = custom_tokenizer(nlp, customize_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = en_core_web_sm.load()\n",
    "\n",
    "# customize_stop_words = {'not':False, 'yeah':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/balatmak/text-preprocessing-steps-and-universal-pipeline\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "customize_stop_words = {'not':False, 'yeah':True}\n",
    "\n",
    "\n",
    "for w,val in customize_stop_words.items():\n",
    "    nlp.vocab[w].is_stop = val\n",
    "\n",
    "\n",
    "class TextPreprocessor():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        TBD\n",
    "        \"\"\"\n",
    "\n",
    "    def preprocess_df(self, df):\n",
    "        return df.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        doc = nlp(text)\n",
    "        #print(\"NLP yeah stop {}\".format(nlp.vocab['yeah'].is_stop))\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return ' '.join([t.lemma_ for t in doc])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(\"../data/raw\") \n",
    "os.chdir(\"/Users/amywinecoff/Documents/CITP/Research/Github/ai-dialogues/startup-study/data/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace some words so all texts refer to them the same way throughout\n",
    "lookp_dict = {\n",
    "    \"artificial intelligence\" : \"ai\", \n",
    "    \"machine learning\" : \"ml\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>pid</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sure, so at companyname we are a software tech...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah. all right, so we do small promotion and ...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yeah. so once a month we co-host a startup int...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeah, so on top of our own solution, we use ai...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeah, sure. there is actually one blog post th...</td>\n",
       "      <td>P1</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote pid   industry\n",
       "0  sure, so at companyname we are a software tech...  P1  Marketing\n",
       "1  yeah. all right, so we do small promotion and ...  P1  Marketing\n",
       "2  yeah. so once a month we co-host a startup int...  P1  Marketing\n",
       "3  yeah, so on top of our own solution, we use ai...  P1  Marketing\n",
       "4  yeah, sure. there is actually one blog post th...  P1  Marketing"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid = 'P1'\n",
    "industry='Marketing'\n",
    "transcript = open('{}.txt'.format(pid), \"r\", encoding='utf-8').read().lower()\n",
    "transcript = contractions.fix(transcript)\n",
    "\n",
    "#This is sloppy, but I couldn't figure out how to get this to work with regex\n",
    "# transcript = transcript.read()\n",
    "# transcript = \" \".join(lookp_dict.get(ele, ele) for ele in transcript.split()).split('Speaker ')\n",
    "# transcript = ''.join([e.replace('\\n', '').replace(\"\\'\", \"\") for e in transcript if len(e)> 0 and e.startswith(\"2:\")])\n",
    "# transcript = contractions.fix(re.sub('2:', ' ', transcript))\n",
    "#print(transcript)\n",
    "\n",
    "transcript_df = pd.DataFrame(transcript.split('*'), columns=['quote'])\n",
    "transcript_df['pid']=pid\n",
    "transcript_df['industry']='Marketing'\n",
    "\n",
    "transcript_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(TextPreprocessor().preprocess_df(transcript_df['quote']))\n",
    "#text.loc[0]['quote']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(transcript)\n",
    "# # #spacy_tokens = [token.text for token in doc]\n",
    "# # #print(spacy_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# # punc_removed = \" \".join([t.text for t in doc if t.text not in string.punctuation])\n",
    "\n",
    "# # spacy_tokens = [t.lemma_ for t in nlp(punc_removed) if not t.is_stop]\n",
    "\n",
    "# spacy_lemmas = [t.lemma_ for t in nlp(transcript) if t.text not in string.punctuation and not t.is_stop]\n",
    "\n",
    "# spacy_text = \" \".join(spacy_lemmas)\n",
    "# #text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\n",
    "# spacy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Note that this current form does remove \"not\" as a stop word, which may be an issue later on\n",
    "\n",
    "# tokens = [t for t in nltk.word_tokenize(text) if t not in string.punctuation and t not in nltk.corpus.stopwords.words('english')]\n",
    "# #print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n",
    "# lemmatized_text = ' '.join(lemmatize_words(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_no_urls = collections.Counter(all_words_no_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
